{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "num of Float : about 50M",
   "id": "7ce93dff00129f67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:32:51.688366Z",
     "start_time": "2024-06-28T10:32:49.526892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ellzaf_ml.models import GhostFaceNetsV2\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "model = GhostFaceNetsV2(image_size=112, width=1.3, dropout=0.)\n",
    "model.eval()\n",
    "input_tensor = torch.randn(1, 3, 112, 112) \n",
    "flops = FlopCountAnalysis(model, input_tensor)\n",
    "print(f\"Total FLOPS: {flops.total()}\")\n",
    "# print(model.output)"
   ],
   "id": "b95aaa2d054e0392",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:32:36.898386Z",
     "start_time": "2024-06-28T10:32:35.890290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ellzaf_ml.models.mobilefacenet import MobileFaceNet\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "model = MobileFaceNet(512)\n",
    "model.eval()\n",
    "input_tensor = torch.randn(1, 3, 112, 112) \n",
    "flops = FlopCountAnalysis(model, input_tensor)\n",
    "print(f\"Total FLOPS: {flops.total()}\")\n",
    "# print(model.output)"
   ],
   "id": "5ffe404d09068deb",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:13:55.490855Z",
     "start_time": "2024-06-28T06:13:50.449535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from ellzaf_ml.models import GhostFaceNetsV2\n",
    "from torchsummary import summary\n",
    "IMAGE_SIZE = 112\n",
    "import time\n",
    "#return embedding\n",
    "model = GhostFaceNetsV2(image_size=IMAGE_SIZE, width=1.3, dropout=0.2)\n",
    "img = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "model =model.cuda()\n",
    "img =img.cuda()\n",
    "model.eval()\n",
    "\n",
    "#return classification\n",
    "# model = GhostFaceNetsV2(image_size=IMAGE_SIZE, num_classes=10, width=1, dropout=0.)\n",
    "# img = torch.randn(3, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n"
   ],
   "id": "b862c570348ea4c8",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T03:42:15.521399Z",
     "start_time": "2024-06-28T03:42:15.215363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=time.time()\n",
    "size = model(img)\n",
    "y=time.time()\n",
    "print(size.size())\n",
    "print(y-x)"
   ],
   "id": "81e563c48252923a",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T01:43:39.395550Z",
     "start_time": "2024-06-28T01:43:39.383525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_data = torch.randn(3, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "print(input_data.dtype)\n",
    "input_data = input_data.half()\n",
    "print(input_data.dtype)"
   ],
   "id": "42977e8998bdb1df",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T01:38:56.233071Z",
     "start_time": "2024-06-28T01:38:55.962015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from losses import CombinedMarginLoss\n",
    "from configs.ghostfacenets import config as cfg\n",
    "import torch\n",
    "margin_loss = CombinedMarginLoss(\n",
    "64,\n",
    "cfg.margin_list[0],\n",
    "cfg.margin_list[1],\n",
    "cfg.margin_list[2],\n",
    "cfg.interclass_filtering_threshold\n",
    ")\n",
    "logits = torch.randn(5, 3)  # Logits ngẫu nhiên\n",
    "labels = torch.randint(0, 3, (5,))\n",
    "print(logits)\n",
    "print(labels)\n",
    "loss = margin_loss(logits, labels)\n",
    "print(loss)"
   ],
   "id": "b8745e6e9f98a445",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T12:45:07.699965Z",
     "start_time": "2024-06-25T12:45:07.473693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "# Example usage:\n",
    "layer = MyLinear(10, 5)\n",
    "print(layer.weight)\n",
    "print(layer.bias)\n"
   ],
   "id": "7e6ba332fa519408",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T16:14:47.854696Z",
     "start_time": "2024-06-25T16:14:47.823447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from margin_model import MarginModel\n",
    "model = MarginModel(512,1000)\n",
    "img = torch.randn((32,512))\n",
    "kq = model(img)\n",
    "print(kq)\n",
    "print(kq.shape)\n",
    "opt = torch.optim.SGD(\n",
    "    params=[{\"params\": model.parameters()}],\n",
    "    lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)\n",
    "\n",
    "print(model.weight)"
   ],
   "id": "311a495fd0b2b0cc",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:21:36.437261Z",
     "start_time": "2024-06-26T05:21:35.884526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "from ellzaf_ml.models.ghostfacenetsv2 import GhostFaceNetsV2\n",
    "model = GhostFaceNetsV2(image_size=112,num_classes=2,  width=1, dropout=0.)\n",
    "model=model.cuda()\n",
    "image = torch.randn([3,3,112,112])\n",
    "image = image.cuda()\n",
    "print(image.shape)\n",
    "kq = model(image)\n",
    "print(kq)\n",
    "# summary(model,(3,112,112))"
   ],
   "id": "b497400d9538eed7",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:37:46.071429Z",
     "start_time": "2024-06-26T07:37:45.783298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ellzaf_ml.models.ghostfacenetsv2 import ModifiedGDC\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "modelGDC = ModifiedGDC(image_size=112, in_chs=960, num_classes=10000, dropout=0.3)\n",
    "modelGDC =modelGDC.cuda()\n",
    "# img = torch.randn((3,960,4,4))\n",
    "# img =img.cuda()\n",
    "# modelGDC(img)\n",
    "summary(modelGDC, (960,4,4)) "
   ],
   "id": "418ff838386c0422",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T06:57:14.781386Z",
     "start_time": "2024-06-26T06:57:14.495200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        self.conv_dw = nn.Conv2d(960, 960, kernel_size=(4, 4), stride=(1, 1), groups=960, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(960, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=False)\n",
    "        self.conv = nn.Conv2d(960, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.linear = nn.Linear(512, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        x = self.bn2(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = CustomModel()\n",
    "\n",
    "# Chuyển mô hình lên GPU nếu có sử dụng CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Sử dụng summary để hiển thị thông tin mô hình và số lượng tham số\n",
    "summary(model, (960, 16, 16))  # Đưa vào kích thước đầu vào (channels, height, width)"
   ],
   "id": "7817321f1cdcfe3d",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "667afb0c5982cef9",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tại sao có sự khác biệt giữa tham số 2 mô hình pytorch và keras được implement trong paper",
   "id": "23883736de97f85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ta có model của keras có tổng tham số là 6880288 (giống trong bài báo)  \n",
    "ở đây có 2 lý do, 1 số lượng tham số của Batchnormalization của Keras gấp  đôi Pytorch =84568  \n",
    "Thứ 2 là số lượng tham số của hàm PReLu trong pytorch chỉ là 1 và trong keras thì tùy từng trường hợp có thể khác rất nhiêu  \n",
    "Tôi đã đếm được model Pytorch sử dụng 41 hàm PRelu và Keras có 11388 tham số của PRelu  \n",
    "Ta có tham số cơ bản của pytorch model là 6,784,373 tham số  \n",
    "Khi chúng ta tính toán và trừ đi thì   \n",
    "6880288 - 6784373 + 41 - 84568 -11388 = 0  \n",
    "đã giải thích được số lượng tham số của 2 phiên bản tại sao khác nhau"
   ],
   "id": "8891e93a0bd0a60a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model=model.cuda()\n",
    "summary(model,(3,112,112))\n",
    "import torch.nn as nn\n",
    "def count_batchnorm_params(model):\n",
    "    total_params = 0\n",
    "    prelu_params = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d) or isinstance(module,nn.BatchNorm1d) :\n",
    "            for param in module.parameters():\n",
    "                total_params += param.numel()\n",
    "        if isinstance(module,nn.PReLU):\n",
    "            prelu_params +=1\n",
    "    print(\"prelu_params\",prelu_params)\n",
    "    return total_params\n",
    "\n",
    "# Gọi hàm và in kết quả\n",
    "num_bn_params = count_batchnorm_params(model)\n",
    "print(f\"Số lượng tham số của tất cả các lớp BatchNorm2d trong mô hình: {num_bn_params}\")"
   ],
   "id": "3972a1ec82983a69",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T03:50:26.855100Z",
     "start_time": "2024-06-28T03:50:26.738515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(*torch.__config__.show().split(\"\\n\"), sep=\"\\n\")"
   ],
   "id": "39844758d1bad47d",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T03:57:33.624946Z",
     "start_time": "2024-06-28T03:57:33.354904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ellzaf_ml.models.mobilefacenet import MobileFaceNet\n",
    "net = MobileFaceNet(512)\n",
    "net.cpu()\n",
    "net.eval()"
   ],
   "id": "ca7d53381390e27f",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:22:12.900360Z",
     "start_time": "2024-06-28T06:22:12.046684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from ellzaf_ml.models import GhostFaceNetsV2\n",
    "from torchsummary import summary\n",
    "IMAGE_SIZE = 112\n",
    "import time\n",
    "#return embedding\n",
    "model = GhostFaceNetsV2(image_size=IMAGE_SIZE, width=1.3, dropout=0.2)\n",
    "img = torch.randn(32, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "model =model.cuda()\n",
    "img =img.cuda()\n",
    "model.eval()\n",
    "\n",
    "# img = img.to_mkldnn()\n",
    "# print(img.shape)\n",
    "# x =time.time()\n",
    "# net(img)\n",
    "# y= time.time()\n",
    "# print(y-x)\n",
    "model.cuda()\n",
    "x =time.time()\n",
    "model(img)\n",
    "y= time.time()\n",
    "print(y-x)\n",
    "\n"
   ],
   "id": "834d34fc05ba703a",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:15:41.524465Z",
     "start_time": "2024-06-28T06:15:41.508833Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c613a900f3f01d15",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T09:07:04.708966Z",
     "start_time": "2024-06-28T09:07:04.437413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from ellzaf_ml.models import GhostFaceNetsV2\n",
    "from torchsummary import summary\n",
    "IMAGE_SIZE = 112\n",
    "import time\n",
    "#return embedding\n",
    "model = GhostFaceNetsV2(image_size=IMAGE_SIZE, width=1.3, dropout=0.2)\n",
    "img = torch.randn(5, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "model = model.cuda()\n",
    "img = img.cuda()\n",
    "model.eval()\n",
    "# model(img)\n",
    "#return classification\n",
    "# model = GhostFaceNetsV2(image_size=IMAGE_SIZE, num_classes=10, width=1, dropout=0.)\n",
    "# img = torch.randn(3, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "x=time.time()\n",
    "size = model(img)\n",
    "y=time.time()\n",
    "print(y-x)\n",
    "print(size.shape)"
   ],
   "id": "e3771f67cabee0b4",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load model Mobilefacenet to test",
   "id": "177e83e39956f253"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T22:31:24.683781Z",
     "start_time": "2024-06-30T22:31:19.715553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ellzaf_ml.models.mobilefacenet import MobileFaceNet\n",
    "import torch\n",
    "device = \"cuda:0\"\n",
    "detect_model = MobileFaceNet(512).to(device)  # embeding size is 512 (feature vector)\n",
    "detect_model.load_state_dict(torch.load('Weights/MobileFace_Net', map_location=lambda storage, loc: storage))"
   ],
   "id": "579699850bb38969",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T22:31:29.856812Z",
     "start_time": "2024-06-30T22:31:25.920888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "summary(detect_model, (3,112,112))"
   ],
   "id": "46ad29b99ebcba61",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "b4b7ec7f3859ef73",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Face alignment test",
   "id": "d16af7918e1a3f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:18:16.748074Z",
     "start_time": "2024-07-02T01:18:16.597135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from MTCNN import create_mtcnn_net_facebank\n",
    "import cv2\n",
    "import numpy as np\n",
    "image_path = \"image/2990.jpg\"\n",
    "\n",
    "img = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "bboxes, landmarks = create_mtcnn_net_facebank(img, 20, \"cpu\",\n",
    "                                                 p_model_path='Weights/pnet_Weights',\n",
    "                                                 r_model_path='Weights/rnet_Weights',\n",
    "                                                 o_model_path='Weights/onet_Weights')\n",
    "print(landmarks)\n",
    "\n"
   ],
   "id": "e3bd98dc6385608d",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:18:19.642145Z",
     "start_time": "2024-07-02T01:18:19.611335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Điểm đặc trưng đã phát hiện, dạng 1D\n",
    "detected_points = landmarks\n",
    "\n",
    "# Tách tọa độ x và y\n",
    "x_coords = detected_points[0, :5]\n",
    "y_coords = detected_points[0, 5:]\n",
    "\n",
    "# Kết hợp x và y thành mảng (5, 2)\n",
    "detected_points_reshaped = np.stack((x_coords, y_coords), axis=1)\n",
    "print(detected_points_reshaped)\n",
    "# Các bộ tọa độ chuẩn đã cho\n",
    "coord5point_1 = np.array([[38.29459953, 51.69630051],\n",
    "                          [73.53179932, 51.50139999],\n",
    "                          [56.02519989, 71.73660278],\n",
    "                          [41.54930115, 92.3655014],\n",
    "                          [70.72990036, 92.20410156]], dtype=np.float32)\n",
    "\n",
    "coord5point_2 = np.array([[30.29459953, 51.69630051],\n",
    "                          [65.53179932, 51.50139999],\n",
    "                          [48.02519989, 71.73660278],\n",
    "                          [33.54930115, 92.3655014],\n",
    "                          [62.72990036, 92.20410156]], dtype=np.float32)\n",
    "print(coord5point_1)\n",
    "\n",
    "# Tính tổng khoảng cách Euclidean giữa các điểm đặc trưng đã phát hiện và từng bộ tọa độ chuẩn\n",
    "distance_1 = np.sum([distance.euclidean(detected_points_reshaped[i], coord5point_1[i]) for i in range(5)])\n",
    "distance_2 = np.sum([distance.euclidean(detected_points_reshaped[i], coord5point_2[i]) for i in range(5)])\n",
    "\n",
    "# In ra tổng khoảng cách để kiểm tra\n",
    "print(\"Tổng khoảng cách với coord5point_1:\", distance_1)\n",
    "print(\"Tổng khoảng cách với coord5point_2:\", distance_2)\n",
    "\n",
    "# So sánh tổng khoảng cách để xác định loại căn chỉnh\n",
    "if distance_1 < distance_2:\n",
    "    print(\"Hình ảnh thuộc loại căn chỉnh thứ nhất (coord5point_1).\")\n",
    "else:\n",
    "    print(\"Hình ảnh thuộc loại căn chỉnh thứ hai (coord5point_2).\")\n"
   ],
   "id": "e3ec0ace002164b6",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "validation on lfw mobile facenet",
   "id": "ab9d70a431deed1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "25755853144fb02b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "64396659868d7aa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "ctx = mx.gpu(args.gpu)"
   ],
   "id": "dd58794c0105e363",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
